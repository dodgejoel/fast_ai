{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Farm Challenge\n",
    "\n",
    "1. Data are prepared in their directories\n",
    "2. What about a way to spin up vgg16 quickly - lets actually save the model somewhere here so we can just load it directly. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense, Dropout\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import image\n",
    "import re\n",
    "import csv\n",
    "from numpy.random import permutation\n",
    "from utils.utils import plots\n",
    "from utils import utils\n",
    "import glob\n",
    "from os import path\n",
    "import os\n",
    "from fast_ai.vgg_setup import make_vgg\n",
    "import itertools\n",
    "\n",
    "pname = 'state_farm_challenge'\n",
    "batch_size = 64\n",
    "proot = '/home/ubuntu/nbs/data/statefarm/'\n",
    "# proot = '/home/ubuntu/nbs/data/statefarm/sample'\n",
    "models_path = path.join(proot, 'models')\n",
    "if not path.exists(models_path):\n",
    "    os.makedirs(models_path)\n",
    "\n",
    "    \n",
    "def do_predict(proot, model):\n",
    "    # load cached test data from disk and predict\n",
    "    # return filenames and predictions\n",
    "    batches = utils.get_batches(path.join(proot, 'test'), shuffle=False)\n",
    "    filenames = batches.filenames\n",
    "    preds = model.predict_generator(batches, batches.nb_sample)\n",
    "    return filenames, preds\n",
    "    \n",
    "    \n",
    "def get_next_filenum(base, tag):\n",
    "    matches = glob.glob(path.join(base, tag + '*'))\n",
    "    trailing_underscore = lambda f: -f[::-1].find('_')\n",
    "    trailing_dot = lambda f: -f[::-1].find('.')\n",
    "    m = 0\n",
    "    for f in matches:\n",
    "        trailing_underscore = -f[::-1].find('_')\n",
    "        trailing_dot = -f[::-1].find('.') - 1\n",
    "        thint = f[trailing_underscore:trailing_dot]\n",
    "        m = max(m, int(thint))\n",
    "    return m + 1\n",
    "    \n",
    "def train_model(model, batches, val_batches, epochs=1, model_tag=pname):\n",
    "    # train the model over these batches (and these validation batches) for this many epochs\n",
    "    # once finished nondestructively save the model to a path dictated by weights_tag parameter\n",
    "    model.fit_generator(\n",
    "        batches,\n",
    "        samples_per_epoch=batches.nb_sample,\n",
    "        nb_epoch=epochs,\n",
    "        validation_data=val_batches,\n",
    "        nb_val_samples=val_batches.nb_sample,    \n",
    "    )\n",
    "    num = get_next_filenum(models_path, model_tag)\n",
    "    fname = '%s_%d.h5' % (model_tag, num)\n",
    "    model.save_weights(path.join(models_path, fname))\n",
    "\n",
    "\n",
    "def display_validation(model, val_batches, num=4):\n",
    "    def plots_idx(idxs, titles, filenames):\n",
    "        if len(idxs):\n",
    "            plots([image.load_img(path.join(proot, 'valid', filenames[i])) for i in idxs], titles=titles)\n",
    "        else:\n",
    "            print('none!')\n",
    "\n",
    "    labels = val_batches.classes\n",
    "    probs = model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "    preds = np.argmax(probs, 1)\n",
    "    filenames = val_batches.filenames\n",
    "\n",
    "    correct = permutation(np.where(preds == labels)[0])[:num]\n",
    "    incorrect = permutation(np.where(preds != labels)[0])[:num]\n",
    "    \n",
    "    print('correct sample')\n",
    "    plots_idx(correct, preds[correct], filenames)\n",
    "    print('incorrect sample')\n",
    "    plots_idx(incorrect, preds[incorrect], filenames)\n",
    "    \n",
    "    for i in range(val_batches.nb_class):\n",
    "        # dogs we got wrong\n",
    "        print('missed from class %d' % i)\n",
    "        miss_idxs = np.where((labels == i) & (labels != preds))[0]\n",
    "        worst_idxs = np.argsort(probs[miss_idxs][:, i])[:num]\n",
    "        missed_dogs_idxs = miss_idxs[worst_idxs]\n",
    "        plots_idx(missed_dogs_idxs, probs[missed_dogs_idxs][:, i], filenames)\n",
    "        \n",
    "        # dogs we got right\n",
    "        print('hits from class %d' % i)\n",
    "        hit_idxs = np.where((labels == i) & (labels == preds))[0]\n",
    "        best_idxs = np.argsort(probs[hit_idxs][:, i])[::-1][:num]\n",
    "        hit_dogs_idxs = hit_idxs[best_idxs]\n",
    "        plots_idx(hit_dogs_idxs, probs[hit_dogs_idxs][:, i], filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finetune(model, classes, lr=.001):\n",
    "    model.pop()\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "def prepare_submission(proot, pname, filenames, predictions, clip=(.05, .95)):\n",
    "    clipped = np.clip(predictions, clip[0], clip[1])\n",
    "    num = get_next_filenum(proot, pname + '_submission')\n",
    "    fname = '%s_submission_%d.csv' % (pname, num)\n",
    "    with open(path.join(proot, fname), 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['img', 'c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "        for filename, ps in itertools.izip(filenames, clipped):\n",
    "            writer.writerow([filename[filename.find('/') + 1:]] + list(ps))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 10 classes.\n",
      "Found 100 images belonging to 10 classes.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 5s - loss: 5.0140 - acc: 0.0500 - val_loss: 3.5165 - val_acc: 0.1200\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 5s - loss: 3.8904 - acc: 0.1600 - val_loss: 2.9305 - val_acc: 0.1400\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 5s - loss: 3.0304 - acc: 0.2800 - val_loss: 2.6863 - val_acc: 0.2300\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "model = make_vgg()\n",
    "finetune(model, 10)\n",
    "train_batches = utils.get_batches(path.join(proot, 'train'))\n",
    "valid_batches = utils.get_batches(path.join(proot, 'valid'), shuffle=False)\n",
    "for _ in range(num_epochs):\n",
    "    train_model(model, train_batches, valid_batches, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = make_vgg()\n",
    "finetune(model, 10)\n",
    "model.load_weights(path.join(models_path, pname + '_3.h5'))\n",
    "\n",
    "#for _ in range(2):\n",
    "#    train_model(remodel, train_batches, valid_batches, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "filenames, predictions = do_predict(proot, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepare_submission(proot, pname, filenames, predictions, clip=(0, 1))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
